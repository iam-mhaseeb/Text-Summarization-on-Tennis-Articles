{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Summarization of Tennis Articles.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "pXyAeLOJ0iuq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Objective"
      ]
    },
    {
      "metadata": {
        "id": "oHKxVo3h0iuu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Tennis fans always try to keep themselves updated with what’s happening in the sport by religiously going through as many online tennis updates as possible. However, this has proven to be a rather difficult job! There are way too many resources and time is a constraint.\n",
        "\n",
        "Therefore, I decided to design a system that could prepare a bullet-point summary for me by scanning through multiple articles. How to go about doing this? That’s what I’ll show you in this tutorial. We will apply the TextRank algorithm on a dataset of scraped articles with the aim of creating a nice and concise summary."
      ]
    },
    {
      "metadata": {
        "id": "rf7CfjW30iuw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ]
    },
    {
      "metadata": {
        "id": "kbxbttpk0iuz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "dd915350-edfb-45e5-a371-66034c149dd8"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import nltk \n",
        "nltk.download('punkt') # one time execution \n",
        "import re\n",
        "import networkx as nx \n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import stopwords \n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MqIn8CmX0iu8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"tennis_articles_v4.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VuvUUv6v0iu_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Exploration"
      ]
    },
    {
      "metadata": {
        "id": "Tm76uJt30ivB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "29d64f90-7cfd-4dcb-e9fd-a545e1836c27"
      },
      "cell_type": "code",
      "source": [
        "df.head() # A quick glimpse of data"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article_id</th>\n",
              "      <th>article_text</th>\n",
              "      <th>source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Maria Sharapova has basically no friends as te...</td>\n",
              "      <td>https://www.tennisworldusa.org/tennis/news/Mar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>BASEL, Switzerland (AP), Roger Federer advance...</td>\n",
              "      <td>http://www.tennis.com/pro-game/2018/10/copil-s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Roger Federer has revealed that organisers of ...</td>\n",
              "      <td>https://scroll.in/field/899938/tennis-roger-fe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Kei Nishikori will try to end his long losing ...</td>\n",
              "      <td>http://www.tennis.com/pro-game/2018/10/nishiko...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Federer, 37, first broke through on tour over ...</td>\n",
              "      <td>https://www.express.co.uk/sport/tennis/1036101...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   article_id                                       article_text  \\\n",
              "0           1  Maria Sharapova has basically no friends as te...   \n",
              "1           2  BASEL, Switzerland (AP), Roger Federer advance...   \n",
              "2           3  Roger Federer has revealed that organisers of ...   \n",
              "3           4  Kei Nishikori will try to end his long losing ...   \n",
              "4           5  Federer, 37, first broke through on tour over ...   \n",
              "\n",
              "                                              source  \n",
              "0  https://www.tennisworldusa.org/tennis/news/Mar...  \n",
              "1  http://www.tennis.com/pro-game/2018/10/copil-s...  \n",
              "2  https://scroll.in/field/899938/tennis-roger-fe...  \n",
              "3  http://www.tennis.com/pro-game/2018/10/nishiko...  \n",
              "4  https://www.express.co.uk/sport/tennis/1036101...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "WIjeODD80ivE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We have 3 columns in our dataset — ‘article_id’, ‘article_text’, and ‘source’. We are most interested in the ‘article_text’ column as it contains the text of the articles. Let’s print the text of the first article in the data just to see how it appears."
      ]
    },
    {
      "metadata": {
        "id": "QWWkLzkY0ivF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "0ca5696b-aaee-4562-82d5-f974009ce950"
      },
      "cell_type": "code",
      "source": [
        "df['article_text'][0]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Maria Sharapova has basically no friends as tennis players on the WTA Tour. The Russian player has no problems in openly speaking about it and in a recent interview she said: 'I don't really hide any feelings too much. I think everyone knows this is my job here. When I'm on the courts or when I'm on the court playing, I'm a competitor and I want to beat every single person whether they're in the locker room or across the net.So I'm not the one to strike up a conversation about the weather and know that in the next few minutes I have to go and try to win a tennis match. I'm a pretty competitive girl. I say my hellos, but I'm not sending any players flowers as well. Uhm, I'm not really friendly or close to many players. I have not a lot of friends away from the courts.' When she said she is not really close to a lot of players, is that something strategic that she is doing? Is it different on the men's tour than the women's tour? 'No, not at all. I think just because you're in the same sport doesn't mean that you have to be friends with everyone just because you're categorized, you're a tennis player, so you're going to get along with tennis players. I think every person has different interests. I have friends that have completely different jobs and interests, and I've met them in very different parts of my life. I think everyone just thinks because we're tennis players we should be the greatest of friends. But ultimately tennis is just a very small part of what we do. There are so many other things that we're interested in, that we do.'\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "AS4hOEea0ivJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing "
      ]
    },
    {
      "metadata": {
        "id": "Ujp3p6Ox0ivJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Split Text into Sentences"
      ]
    },
    {
      "metadata": {
        "id": "VphBZhup0ivJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "497a805b-1048-4d05-e31d-96ef00704c96"
      },
      "cell_type": "code",
      "source": [
        "sentences = [] \n",
        "for s in df['article_text']: \n",
        "  sentences.append(sent_tokenize(s))\n",
        "# flatten list\n",
        "sentences = [y for x in sentences for y in x]\n",
        "sentences[:5]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Maria Sharapova has basically no friends as tennis players on the WTA Tour.',\n",
              " \"The Russian player has no problems in openly speaking about it and in a recent interview she said: 'I don't really hide any feelings too much.\",\n",
              " 'I think everyone knows this is my job here.',\n",
              " \"When I'm on the courts or when I'm on the court playing, I'm a competitor and I want to beat every single person whether they're in the locker room or across the net.So I'm not the one to strike up a conversation about the weather and know that in the next few minutes I have to go and try to win a tennis match.\",\n",
              " \"I'm a pretty competitive girl.\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "l8y5gB0_0ivN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Download GloVe Word Embeddings"
      ]
    },
    {
      "metadata": {
        "id": "HvUcttNH0ivO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "GloVe word embeddings are vector representation of words. These word embeddings will be used to create vectors for our sentences. We could have also used the Bag-of-Words or TF-IDF approaches to create features for our sentences, but these methods ignore the order of the words (and the number of features is usually pretty large).\n",
        "\n",
        "We will be using the pre-trained Wikipedia 2014 + Gigaword 5 GloVe vectors available here. Heads up — the size of these word embeddings is 822 MB."
      ]
    },
    {
      "metadata": {
        "id": "t_kt-SGj0ivO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "d7ee1a9b-147d-4d47-800c-e925e8066571"
      },
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip \n",
        "!unzip glove*.zip"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-12-30 15:54:59--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2018-12-30 15:54:59--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  43.7MB/s    in 14s     \n",
            "\n",
            "2018-12-30 15:55:14 (57.3 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "N0kFmUuk001-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Extract word vectors \n",
        "word_embeddings = {} \n",
        "f = open('glove.6B.100d.txt', encoding='utf-8') \n",
        "for line in f: \n",
        "    values = line.split() \n",
        "    word = values[0] \n",
        "    coefs = np.asarray(values[1:], dtype='float32')   \n",
        "    word_embeddings[word] = coefs \n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hSDt-8781NAz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We now have word vectors for 400,000 different terms stored in the dictionary — ‘word_embeddings’."
      ]
    },
    {
      "metadata": {
        "id": "tSO3q-Lf1eiQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It is always a good practice to make your textual data noise-free as much as possible. So, let’s do some basic text cleaning."
      ]
    },
    {
      "metadata": {
        "id": "HPDPPC-K1H9R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# remove punctuations, numbers and special characters \n",
        "clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \") \n",
        "# make alphabets lowercase \n",
        "clean_sentences = [s.lower() for s in clean_sentences]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZNGwsShO1m3k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Get rid of the stopwords (commonly used words of a language — is, am, the, of, in, etc.) present in the sentences. If you have not downloaded nltk-stopwords, then execute the following line of code:"
      ]
    },
    {
      "metadata": {
        "id": "bx2E_2Iz1nTJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "14f5953e-396e-45ac-d011-c6036f3c7525"
      },
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words('english')\n",
        "# function to remove stopwords \n",
        "def remove_stopwords(sen):     \n",
        "    sen_new = \" \".join([i for i in sen if i not in stop_words])          \n",
        "    return sen_new"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "X76yG9LD1yNg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# remove stopwords from the sentences \n",
        "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EHyWwa-C16W7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Vector Representation of Sentences"
      ]
    },
    {
      "metadata": {
        "id": "ZtWrcl5a18sA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Extract word vectors \n",
        "word_embeddings = {} \n",
        "f = open('glove.6B.100d.txt', encoding='utf-8') \n",
        "for line in f: \n",
        "    values = line.split() \n",
        "    word = values[0] \n",
        "    coefs = np.asarray(values[1:], dtype='float32')    \n",
        "    word_embeddings[word] = coefs \n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OgodFb8L2HGq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, let’s create vectors for our sentences. We will first fetch vectors (each of size 100 elements) for the constituent words in a sentence and then take mean/average of those vectors to arrive at a consolidated vector for the sentence."
      ]
    },
    {
      "metadata": {
        "id": "rH4OBrOm2Hqn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sentence_vectors = [] \n",
        "for i in clean_sentences: \n",
        "  if len(i) != 0: \n",
        "    v = sum([word_embeddings.get(w, np.zeros((100,))) for w in  \n",
        "        i.split()])/(len(i.split())+0.001) \n",
        "  else: \n",
        "    v = np.zeros((100,)) \n",
        "  sentence_vectors.append(v)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PZxp1A6V2L3o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Similarity Matrix Preparation"
      ]
    },
    {
      "metadata": {
        "id": "jHsIZkxl2Ref",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The next step is to find similarities between the sentences, and we will use the cosine similarity approach for this challenge. Let’s create an empty similarity matrix for this task and populate it with cosine similarities of the sentences.\n",
        "\n",
        "Let’s first define a zero matrix of dimensions (n * n). We will initialize this matrix with cosine similarity scores of the sentences. Here, n is the number of sentences."
      ]
    },
    {
      "metadata": {
        "id": "_OQP3HCk2R3J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# similarity matrix \n",
        "sim_mat = np.zeros([len(sentences), len(sentences)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rQATbIOX2TlF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(len(sentences)): \n",
        "  for j in range(len(sentences)): \n",
        "    if i != j: \n",
        "      sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), \n",
        "                                        sentence_vectors[j].reshape(1,100))[0,0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dJeILx-G2xon",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Applying PageRank Algorithm"
      ]
    },
    {
      "metadata": {
        "id": "PrH5ccey24b7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Before proceeding further, let’s convert the similarity matrix sim_mat into a graph. The nodes of this graph will represent the sentences and the edges will represent the similarity scores between the sentences. On this graph, we will apply the PageRank algorithm to arrive at the sentence rankings."
      ]
    },
    {
      "metadata": {
        "id": "qqybqBqy244t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nx_graph = nx.from_numpy_array(sim_mat) \n",
        "scores = nx.pagerank(nx_graph)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g5UTDS2u3H3l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Summary Extraction"
      ]
    },
    {
      "metadata": {
        "id": "Uc9dk2B23EDK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "457dbee8-afb7-4726-f6ac-54ad2ddfa844"
      },
      "cell_type": "code",
      "source": [
        "ranked_sentences = sorted(((scores[i],s) for i,s in \n",
        "                           enumerate(sentences)), reverse=True)\n",
        "# Extract top 10 sentences as the summary \n",
        "for i in range(10): \n",
        "  print(ranked_sentences[i][1])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "When I'm on the courts or when I'm on the court playing, I'm a competitor and I want to beat every single person whether they're in the locker room or across the net.So I'm not the one to strike up a conversation about the weather and know that in the next few minutes I have to go and try to win a tennis match.\n",
            "Major players feel that a big event in late November combined with one in January before the Australian Open will mean too much tennis and too little rest.\n",
            "Speaking at the Swiss Indoors tournament where he will play in Sundays final against Romanian qualifier Marius Copil, the world number three said that given the impossibly short time frame to make a decision, he opted out of any commitment.\n",
            "\"I felt like the best weeks that I had to get to know players when I was playing were the Fed Cup weeks or the Olympic weeks, not necessarily during the tournaments.\n",
            "Currently in ninth place, Nishikori with a win could move to within 125 points of the cut for the eight-man event in London next month.\n",
            "He used his first break point to close out the first set before going up 3-0 in the second and wrapping up the win on his first match point.\n",
            "The Spaniard broke Anderson twice in the second but didn't get another chance on the South African's serve in the final set.\n",
            "\"We also had the impression that at this stage it might be better to play matches than to train.\n",
            "The competition is set to feature 18 countries in the November 18-24 finals in Madrid next year, and will replace the classic home-and-away ties played four times per year for decades.\n",
            "Federer said earlier this month in Shanghai in that his chances of playing the Davis Cup were all but non-existent.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dWDeO2kj3MxG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And there we go! An awesome, neat, concise, and useful summary for our articles."
      ]
    },
    {
      "metadata": {
        "id": "_y6nt6MS3W7u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Conclusion"
      ]
    },
    {
      "metadata": {
        "id": "6c_nNA7h3Zrt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Text Summarization has a variety of use cases and has spawned extremely successful applications. Whether it’s for leveraging in your business, or just for your own knowledge, text summarization is an approach all NLP enthusiasts should be familiar with."
      ]
    }
  ]
}